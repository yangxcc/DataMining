# TF/IDF是一种统计方法，用于评估一字词对于一个文件集或一个语料库中的一份文件的重要程度
'''
关键技术
（1）正则表达式：是一种用来匹配字符串文本的强有力武器，它使用一种描述性的语言来给字符串定义一个规则
正则表达式通常用于在文本中查找匹配的字符串
Python提供了re模块，包含了所有正则表达式的功能
re.match(pattern,string,flags)  第一个参数是正则表达式，第二个参数表示要匹配的字符串，第三个
参数是标志位，用于控制正则表达式的匹配方式，例如是否区分大小写、多行匹配等
match总是从开头开始匹配
re.split()分组，用正则表达式切分字符串
re.search()将对整个字符串进行搜索，并返回第一个匹配的字符串的match对象
re.findall()函数将返回一个所有匹配字符串的字符串列表
（2）中文分词
中文分词就是将连续的子序列按照一定的规范重新组合成词序列的过程。中文分词是网页分析索引的基础
jieba是一个支持中文分词、高准确率、高效率的python中文分词组件，它支持繁体分词和自定义字典，并支持三种分词模式：
i.精确模式：试图把句子最精确的切开，适合文本分析
ii.全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义的问题
iii.搜索引擎模式：在精确模式的基础上对长词再次切分，提高召回率，适合用于搜索引擎分词
jieba.cut()   第一个参数为需要分词的字符串  cut_all参数用来控制分词模式,默认为精确模式
jieba.cut_for_search()搜索引擎模式
jieba.cut()返回的是一个可迭代的生成器类型，可用for循环来得到各个词语
join()主要用于把字符串、字典、元组、列表中的元素使用指定的分隔符连接，从而形成新的元素及表现形式
为jieba添加自定义字典  jieba.load_userdict(filename)  #filename为自定义字典的路径
自定义字典示例：乾清宫 5 ns
               黄琉璃瓦 4
               云计算 3
               李孝妇 2 nr
词典格式是每次占一行，每行三部分，第一部分为词语，第二部分为词频，第三部分为词性
ns为地点名词 nz为其他专有名词 a是形容词 v是动词 d是副词
（3）文本分类的关键词提取
首先需要import jieba.analyse
jieba.analyse.extract_tags(sentence,topK=20,withWeight=False,allowPOS=())
sentence为待提取的文本；topK未返回几个权重最大的关键词，默认值为20，withWeight为是否一并返回
关键词权重，默认值为false，allowPOS指仅包含指定词性的词，默认值为空，即不进行筛选
（4）deque双向队列类似于list列表，位于python标准库的collections中，它提供了两端都可以操作的序列
这意味着在序列的前后端都可以执行添加或者删除的操作
from collections import deque
d=deque()  当参数为maxlength=xx时，表示队列的最大长度
d.append()   #添加元素
假设d=deque(['1','2','3','4'])
d.pop()  抛出的是4   d.popleft() 抛出的是1
d=deque([1,2,3,4,5])
d.extend([0])
此时d=deque([1,2,3,4,5,0])
d.extendleft([6,7,8])
此时d=deque([8,7,6,1,2,3,4,5,0])
'''

'''
网络爬虫的实现原理和过程
（1）获取的初始url,初始的url地址可以由用户指定的某个或某几个初始爬取网页决定
（2）根据初始的URL爬取页面并获得新的URL，在获得初始的url地址后，首先需要爬取对应url地址中的网页，
在爬取了对应的url地址中的网页后将网页存储到原始数据库中，并且在爬取网页的同时发现新的url地址，将以爬取的
url地址存放到一个已爬URL列表中，用于去重复判断爬取的进程
（3）将新的URL放到URL队列中
（4）从URL队列中读取新的URL，并依据新的URL爬取网页，同时从新的网页中获取新的URL，并重复上述的爬取过程
（5）当满足爬虫系统设置的停止条件时停止爬取。如果没有设置停止条件，则爬虫程序会一直进行下去，直到无法获取新的URL为止
'''